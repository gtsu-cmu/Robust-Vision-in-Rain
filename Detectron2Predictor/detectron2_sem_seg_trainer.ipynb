{"cells":[{"cell_type":"markdown","metadata":{"id":"-4OJq6c4oPqR"},"source":["# Google Colab initialization"],"id":"-4OJq6c4oPqR"},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":611,"status":"ok","timestamp":1650042373698,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"868f1a31-1da7-479c-9bd0-35eabef2ad1d"},"outputs":[],"source":["GOOGLE_COLAB = True\n","# GOOGLE_COLAB = False"],"id":"868f1a31-1da7-479c-9bd0-35eabef2ad1d"},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1650042375464,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"7150dd93"},"outputs":[],"source":["# if GOOGLE_COLAB == True:\n","#     !pip install pyyaml==5.1\n","\n","#     import torch\n","#     TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","#     CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","#     print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n","#     # Install detectron2 that matches the above pytorch version\n","#     # See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","#     !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html > /dev/null \n","#     # If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n","\n","#     exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"],"id":"7150dd93"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3252,"status":"ok","timestamp":1650042378706,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"73d376c0","outputId":"eb31b952-969e-4a8c-f54f-eac8dd9f4072"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["if GOOGLE_COLAB == True:\n","    from google.colab import drive\n","    drive.mount('/content/drive/')"],"id":"73d376c0"},{"cell_type":"code","source":["# if GOOGLE_COLAB == True:\n","#     !ls '/content/drive/MyDrive/18744/data/Cityscapes/'\n","\n","#     !mkdir '/content/data/'\n","#     !mkdir '/content/data/Cityscapes/'\n","\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Cityscapes/gtFine_trainvaltest.zip' -d '/content/data/Cityscapes/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Cityscapes/leftImg8bit_trainvaltest.zip' -d '/content/data/Cityscapes/' > /dev/null\n","#     !tar -xf '/content/drive/MyDrive/18744/data/Cityscapes/mapped_labels.tar' --skip-old-files --directory '/content/data/Cityscapes/'\n","\n","#     !ls '/content/drive/MyDrive/18744/data/Carla/packaging/'\n","\n","#     !mkdir '/content/data/'\n","#     !mkdir '/content/data/Carla/'\n","#     !mkdir '/content/data/Carla/packaging/'\n","\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package2.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package3.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package4.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package5.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package6.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package7.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package8.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !unzip -n '/content/drive/MyDrive/18744/data/Carla/packaging/package9.zip' -d '/content/data/Carla/packaging/' > /dev/null\n","#     !tar -xf '/content/drive/MyDrive/18744/data/Carla/packaging/semantic_train.tar' --skip-old-files --directory '/content/data/Carla/packaging/'\n","\n","#     !ls '/content/drive/MyDrive/18744/data/RainAddition/train/'\n","\n","#     !mkdir '/content/data/'\n","#     !mkdir '/content/data/RainAddition/'\n","#     !mkdir '/content/data/RainAddition/train/'\n","\n","#     !tar -xf '/content/drive/MyDrive/18744/data/RainAddition/train/1.tar' --skip-old-files --directory '/content/data/RainAddition/train/'\n","#     !tar -xf '/content/drive/MyDrive/18744/data/RainAddition/train/2.tar' --skip-old-files --directory '/content/data/RainAddition/train/'\n","#     !tar -xf '/content/drive/MyDrive/18744/data/RainAddition/train/3.tar' --skip-old-files --directory '/content/data/RainAddition/train/'"],"metadata":{"id":"T1wp5irN5qw9","executionInfo":{"status":"ok","timestamp":1650042378707,"user_tz":240,"elapsed":19,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"}}},"id":"T1wp5irN5qw9","execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1650042378708,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"2854f46b","outputId":"1ccfec6e-a24a-4381-cfa3-d79f3dd4b96b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Carla  Cityscapes  RainAddition\n","/content/drive/MyDrive/18744/Robust-Vision-in-Rain/Detectron2Predictor\n","classes.txt\t\t\t    detectron2_sem_seg_trainer.py\n","classes.xlsx\t\t\t    evaluation.xlsx\n","configs\t\t\t\t    __init__.py\n","data_time_gdrive_vs_local.txt\t    output\n","detectron2_predictor.ipynb\t    output_all\n","detectron2_predictor.py\t\t    __pycache__\n","detectron2_real_vs_simulated.ipynb  tensorboard.ipynb\n","detectron2_sem_seg_trainer.ipynb    utilities.py\n"]}],"source":["if GOOGLE_COLAB == True:\n","    !ls '/content/drive/MyDrive/18744/data/'\n","    %cd '/content/drive/MyDrive/18744/Robust-Vision-in-Rain/Detectron2Predictor/'\n","    !ls"],"id":"2854f46b"},{"cell_type":"markdown","metadata":{"id":"vOI__nJ2oiYZ"},"source":["# Libraries"],"id":"vOI__nJ2oiYZ"},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3456,"status":"ok","timestamp":1650042382156,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"566a15a0"},"outputs":[],"source":["main_dir = './'\n","data_dir = '/home/tunx404/Miscellaneous/data/' # Local Jupyter\n","\n","if GOOGLE_COLAB == True:\n","    # data_dir = '/content/drive/MyDrive/18744/data/'\n","    data_dir = '/content/data/'\n","\n","\"\"\"Detectron2 heads\n","\n","ObjectDetection\n","SemanticSegmentation\n","InstanceSegmentation\n","PanopticSegmentation\n","\"\"\"\n","\n","from detectron2_predictor import Detectron2Predictor\n","\n","from utilities import create_file_list, imshow_jupyter\n","# import carla_converter\n","\n","import os\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","import torch\n","import numpy as np\n","\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","if GOOGLE_COLAB:\n","    from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","\n","from detectron2.engine import DefaultTrainer, hooks\n","from detectron2.evaluation import SemSegEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","\n","from tqdm import tqdm"],"id":"566a15a0"},{"cell_type":"markdown","metadata":{"id":"WG-v-IBXo7e7"},"source":["# List images & get annotations"],"id":"WG-v-IBXo7e7"},{"cell_type":"markdown","metadata":{"id":"bJEp7LRapA9e"},"source":["## Carla"],"id":"bJEp7LRapA9e"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1650042382157,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"16e51555","outputId":"c7487bb6-0988-4c93-a791-a2132c0b185f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images: 2990\n","Number of images: 1173\n","**************************************************\n","Carla clear\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144063.4320533_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144076.9520714_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_semantic_train.png'}\n","2990\n","1173\n","\n","**************************************************\n","Carla rain\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_rain_S.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_rain_H.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_rain_H.png', 'height': 720, 'width': 1280, 'image_id': '1647144063.4320533_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_rain_S.png', 'height': 720, 'width': 1280, 'image_id': '1647144076.9520714_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_semantic_train.png'}\n","2990\n","1173\n","\n","**************************************************\n","Carla clear + rain\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_rain_S.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_rain_H.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","5980\n","2346\n"]}],"source":["data_carla_dir = data_dir + 'Carla/packaging/'\n","\n","# Dir structure:\n","# <data_carla_dir>\n","#     packages2\n","#     packages3\n","#     ...\n","\n","def get_carla_file_list(data_dir, packages=[], levels=[]):\n","    file_list = []\n","    \n","    for package in packages:\n","        temp_file_name_list, temp_file_path_list = create_file_list(data_dir, package)\n","        \n","        for i in range(len(temp_file_name_list)):\n","            file_name_split = temp_file_name_list[i].split('_')\n","            if len(file_name_split) == 3: # [id, type, level.png]\n","                file_id = file_name_split[0]\n","                level = file_name_split[2].replace('.png', '')\n","                if level in levels:\n","                    file_list.append((file_id, level, package))\n","                    \n","    print(f'Number of images: {len(file_list)}')\n","    \n","    return file_list\n","\n","data_carla_train_file_list = get_carla_file_list(data_carla_dir, packages=['package2', 'package3', 'package4', 'package5', 'package6', 'package7', 'package9'], levels=['H', 'M', 'S'])\n","data_carla_val_file_list = get_carla_file_list(data_carla_dir, packages=['package8'], levels=['H', 'M', 'S'])\n","\n","# # Toy example\n","# data_carla_train_file_list = data_carla_train_file_list[:10]\n","# data_carla_val_file_list = data_carla_val_file_list[:10]\n","\n","def get_carla_dicts(file_list, data_dir, clear=True, rain=True):\n","    dicts = []\n","    \n","    for file in file_list:\n","        file_id, level, package = file\n","\n","        image_clear_path = os.path.join(data_dir, package, file_id + '_clear.png')\n","        image_rain_path = os.path.join(data_dir, package, file_id + '_rain_' + level + '.png')\n","        # image_semantic_path = os.path.join(data_dir, package, file_id + '_semantic_single.png') # Carla ID\n","        image_semantic_path = os.path.join(data_dir, package, file_id + '_semantic_train.png') # Train ID\n","\n","        if clear == True:\n","            record = {}\n","            record['file_name'] = image_clear_path\n","            record['height'] = 720 # shape[0]\n","            record['width'] = 1280 # shape[1]\n","            record['image_id'] = file_id + '_clear'\n","            record['sem_seg_file_name'] = image_semantic_path\n","            dicts.append(record)\n","        \n","        if rain == True:\n","            record = {}\n","            record['file_name'] = image_rain_path\n","            record['height'] = 720 # shape[0]\n","            record['width'] = 1280 # shape[1]\n","            record['image_id'] = file_id + '_rain'\n","            record['sem_seg_file_name'] = image_semantic_path\n","            dicts.append(record)\n","\n","    return dicts\n","\n","print('**************************************************')\n","print('Carla clear')\n","def get_carla_clear_train_dicts():\n","    return get_carla_dicts(data_carla_train_file_list, data_carla_dir, clear=True, rain=False)\n","\n","def get_carla_clear_val_dicts():\n","    return get_carla_dicts(data_carla_val_file_list, data_carla_dir, clear=True, rain=False)\n","\n","for dict in get_carla_clear_train_dicts()[:4]:\n","    print(dict)\n","    \n","print(len(get_carla_clear_train_dicts())) # 2990\n","print(len(get_carla_clear_val_dicts()))   # 1173\n","\n","print('\\n**************************************************')\n","print('Carla rain')\n","def get_carla_rain_train_dicts():\n","    return get_carla_dicts(data_carla_train_file_list, data_carla_dir, clear=False, rain=True)\n","\n","def get_carla_rain_val_dicts():\n","    return get_carla_dicts(data_carla_val_file_list, data_carla_dir, clear=False, rain=True)\n","\n","for dict in get_carla_rain_train_dicts()[:4]:\n","    print(dict)\n","    \n","print(len(get_carla_rain_train_dicts())) # 2990\n","print(len(get_carla_rain_val_dicts()))   # 1173\n","\n","print('\\n**************************************************')\n","print('Carla clear + rain')\n","def get_carla_all_train_dicts():\n","    return get_carla_dicts(data_carla_train_file_list, data_carla_dir, clear=True, rain=True)\n","\n","def get_carla_all_val_dicts():\n","    return get_carla_dicts(data_carla_val_file_list, data_carla_dir, clear=True, rain=True)\n","\n","for dict in get_carla_all_train_dicts()[:4]:\n","    print(dict)\n","    \n","print(len(get_carla_all_train_dicts())) # 2990*2 = 5980\n","print(len(get_carla_all_val_dicts()))   # 1173*2 = 2346"],"id":"16e51555"},{"cell_type":"markdown","metadata":{"id":"CEjETjkApDCI"},"source":["## Cityscapes"],"id":"CEjETjkApDCI"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1650042382157,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"0c8d50c8-2f3a-4e2e-9d35-1a8e7d47d7b8","outputId":"610234d7-6c89-48d7-ada3-3ef98c07b8cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of cities: 18\n","Number of images: 2276\n","Number of images: 699\n","**************************************************\n","Cityscapes clear\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/aachen/aachen_000000_000019_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000000_000019_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000000_000019_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/aachen/aachen_000001_000019_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000001_000019_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000001_000019_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/aachen/aachen_000002_000019_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000002_000019_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000002_000019_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/aachen/aachen_000003_000019_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000003_000019_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000003_000019_train.png'}\n","2276\n","699\n","\n","**************************************************\n","Cityscapes rain\n","{'file_name': '/content/data/RainAddition/train/aachen/aachen_000000_000019_H.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000000_000019_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000000_000019_train.png'}\n","{'file_name': '/content/data/RainAddition/train/aachen/aachen_000001_000019_M.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000001_000019_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000001_000019_train.png'}\n","{'file_name': '/content/data/RainAddition/train/aachen/aachen_000002_000019_S.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000002_000019_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000002_000019_train.png'}\n","{'file_name': '/content/data/RainAddition/train/aachen/aachen_000003_000019_H.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000003_000019_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000003_000019_train.png'}\n","2276\n","699\n","\n","**************************************************\n","Cityscapes clear + rain\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/aachen/aachen_000000_000019_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000000_000019_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000000_000019_train.png'}\n","{'file_name': '/content/data/RainAddition/train/aachen/aachen_000000_000019_H.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000000_000019_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000000_000019_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/aachen/aachen_000001_000019_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000001_000019_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000001_000019_train.png'}\n","{'file_name': '/content/data/RainAddition/train/aachen/aachen_000001_000019_M.png', 'height': 1024, 'width': 2048, 'image_id': 'aachen_000001_000019_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/aachen/aachen_000001_000019_train.png'}\n","4552\n","1398\n"]}],"source":["data_dir_cityscapes = data_dir + 'Cityscapes/leftImg8bit/train/'\n","data_dir_rain_addition = data_dir + 'RainAddition/train/'\n","# anno_dir_cityscapes = data_dir + 'Cityscapes/gtFine/train/'\n","anno_dir_cityscapes = data_dir + 'Cityscapes/mapped_labels/train/'\n","\n","city_name_list, _ = create_file_list(data_dir_cityscapes)\n","print(f'Number of cities: {len(city_name_list)}')\n","\n","def get_cityscapes_file_list(data_dir, cities=[], levels=[]):\n","    file_list = []\n","    \n","    for city in cities:\n","        temp_file_name_list, _ = create_file_list(data_dir, city)\n","        \n","        for file_name in temp_file_name_list:\n","            file_list.append((file_name, city))\n","                    \n","    print(f'Number of images: {len(file_list)}')\n","    \n","    return file_list\n","\n","data_cityscapes_train_file_list = get_cityscapes_file_list(data_dir_cityscapes, cities=city_name_list[:13], levels=['H', 'M', 'S'])\n","data_cityscapes_val_file_list   = get_cityscapes_file_list(data_dir_cityscapes, cities=city_name_list[13:], levels=['H', 'M', 'S'])\n","\n","# # Toy example\n","# data_cityscapes_train_file_list = data_cityscapes_train_file_list[:10]\n","# data_cityscapes_val_file_list = data_cityscapes_val_file_list[:10]\n","\n","def get_cityscapes_dicts(file_list, data_dir_main, data_dir_rain, anno_dir, clear=True, rain=True, levels=[]):\n","    dicts = []\n","    \n","    # for file in file_list:\n","    for index, file in enumerate(file_list):\n","        file_name, city = file\n","\n","        image_clear_path = os.path.join(data_dir_main, city, file_name)\n","        \n","        file_name_split = file_name.split('_')\n","        # anno_name = file_name_split[0] + '_' + file_name_split[1] + '_' + file_name_split[2] + '_gtFine_labelIds.png' # Default\n","        image_id = file_name_split[0] + '_' + file_name_split[1] + '_' + file_name_split[2]\n","        anno_name = image_id + '_train.png' # Mapped\n","        image_semantic_path = os.path.join(anno_dir, city, anno_name)\n","\n","        if clear == True:\n","            record = {}\n","            record['file_name'] = image_clear_path\n","            record['height'] = 1024 # shape[0]\n","            record['width'] = 2048 # shape[1]\n","            record['image_id'] = image_id + '_clear'\n","            record['sem_seg_file_name'] = image_semantic_path\n","            dicts.append(record)\n","\n","        if rain == True:\n","            num_levels = len(levels)\n","            # for level in levels: # All 3 levels\n","            for level in [levels[index%num_levels]]: # 1 level\n","                image_rain_name = image_id + '_' + level + '.png'\n","                image_rain_path = os.path.join(data_dir_rain, city, image_rain_name)\n","\n","                record = {}\n","                record['file_name'] = image_rain_path\n","                record['height'] = 1024\n","                record['width'] = 2048\n","                record['image_id'] = image_id + '_rain'\n","                record['sem_seg_file_name'] = image_semantic_path\n","                dicts.append(record)\n","\n","    return dicts\n","\n","print('**************************************************')\n","print('Cityscapes clear')\n","def get_cityscapes_clear_train_dicts():\n","    return get_cityscapes_dicts(data_cityscapes_train_file_list, data_dir_cityscapes, data_dir_rain_addition, anno_dir_cityscapes, clear=True, rain=False, levels=['H', 'M', 'S'])\n","\n","def get_cityscapes_clear_val_dicts():\n","    return get_cityscapes_dicts(data_cityscapes_val_file_list, data_dir_cityscapes, data_dir_rain_addition, anno_dir_cityscapes, clear=True, rain=False, levels=['H', 'M', 'S'])\n","\n","for dict in get_cityscapes_clear_train_dicts()[:4]:\n","    print(dict)\n","    \n","print(len(get_cityscapes_clear_train_dicts())) # 2276\n","print(len(get_cityscapes_clear_val_dicts()))   # 699\n","\n","print('\\n**************************************************')\n","print('Cityscapes rain')\n","def get_cityscapes_rain_train_dicts():\n","    return get_cityscapes_dicts(data_cityscapes_train_file_list, data_dir_cityscapes, data_dir_rain_addition, anno_dir_cityscapes, clear=False, rain=True, levels=['H', 'M', 'S'])\n","\n","def get_cityscapes_rain_val_dicts():\n","    return get_cityscapes_dicts(data_cityscapes_val_file_list, data_dir_cityscapes, data_dir_rain_addition, anno_dir_cityscapes, clear=False, rain=True, levels=['H', 'M', 'S'])\n","\n","for dict in get_cityscapes_rain_train_dicts()[:4]:\n","    print(dict)\n","    \n","print(len(get_cityscapes_rain_train_dicts())) # 2276\n","print(len(get_cityscapes_rain_val_dicts()))   # 699\n","\n","print('\\n**************************************************')\n","print('Cityscapes clear + rain')\n","def get_cityscapes_all_train_dicts():\n","    return get_cityscapes_dicts(data_cityscapes_train_file_list, data_dir_cityscapes, data_dir_rain_addition, anno_dir_cityscapes, clear=True, rain=True, levels=['H', 'M', 'S'])\n","\n","def get_cityscapes_all_val_dicts():\n","    return get_cityscapes_dicts(data_cityscapes_val_file_list, data_dir_cityscapes, data_dir_rain_addition, anno_dir_cityscapes, clear=True, rain=True, levels=['H', 'M', 'S'])\n","\n","for dict in get_cityscapes_all_train_dicts()[:4]:\n","    print(dict)\n","    \n","print(len(get_cityscapes_all_train_dicts())) # 2276*2 = 4552\n","print(len(get_cityscapes_all_val_dicts()))   # 699*2  = 1398"],"id":"0c8d50c8-2f3a-4e2e-9d35-1a8e7d47d7b8"},{"cell_type":"markdown","metadata":{"id":"0gefPQJF9bmn"},"source":["## Combined"],"id":"0gefPQJF9bmn"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wECyKDe9bCj","executionInfo":{"status":"ok","timestamp":1650042383582,"user_tz":240,"elapsed":1436,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"}},"outputId":"f6452a8a-e5b1-4635-c9e3-4095df28fbc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["**************************************************\n","Combined clear\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144063.4320533_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144076.9520714_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_semantic_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/strasbourg/strasbourg_000001_064224_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_064224_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_064224_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/strasbourg/strasbourg_000001_064393_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_064393_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_064393_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/strasbourg/strasbourg_000001_065214_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065214_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065214_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/strasbourg/strasbourg_000001_065572_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065572_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065572_train.png'}\n","5266\n","1872\n","\n","**************************************************\n","Combined rain\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_rain_S.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_rain_H.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_rain_H.png', 'height': 720, 'width': 1280, 'image_id': '1647144063.4320533_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144063.4320533_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_rain_S.png', 'height': 720, 'width': 1280, 'image_id': '1647144076.9520714_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144076.9520714_semantic_train.png'}\n","{'file_name': '/content/data/RainAddition/train/strasbourg/strasbourg_000001_064224_M.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_064224_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_064224_train.png'}\n","{'file_name': '/content/data/RainAddition/train/strasbourg/strasbourg_000001_064393_S.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_064393_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_064393_train.png'}\n","{'file_name': '/content/data/RainAddition/train/strasbourg/strasbourg_000001_065214_H.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065214_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065214_train.png'}\n","{'file_name': '/content/data/RainAddition/train/strasbourg/strasbourg_000001_065572_M.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065572_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065572_train.png'}\n","5266\n","1872\n","\n","**************************************************\n","Combined clear + rain\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_rain_S.png', 'height': 720, 'width': 1280, 'image_id': '1647144040.0070002_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144040.0070002_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_clear.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_clear', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","{'file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_rain_H.png', 'height': 720, 'width': 1280, 'image_id': '1647144051.8223236_rain', 'sem_seg_file_name': '/content/data/Carla/packaging/package2/1647144051.8223236_semantic_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/strasbourg/strasbourg_000001_065214_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065214_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065214_train.png'}\n","{'file_name': '/content/data/RainAddition/train/strasbourg/strasbourg_000001_065214_H.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065214_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065214_train.png'}\n","{'file_name': '/content/data/Cityscapes/leftImg8bit/train/strasbourg/strasbourg_000001_065572_leftImg8bit.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065572_clear', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065572_train.png'}\n","{'file_name': '/content/data/RainAddition/train/strasbourg/strasbourg_000001_065572_M.png', 'height': 1024, 'width': 2048, 'image_id': 'strasbourg_000001_065572_rain', 'sem_seg_file_name': '/content/data/Cityscapes/mapped_labels/train/strasbourg/strasbourg_000001_065572_train.png'}\n","10532\n","3744\n"]}],"source":["print('**************************************************')\n","print('Combined clear')\n","def get_combined_clear_train_dicts():\n","    return get_carla_clear_train_dicts() + get_cityscapes_clear_train_dicts()\n","\n","def get_combined_clear_val_dicts():\n","    return get_carla_clear_val_dicts() + get_cityscapes_clear_val_dicts()\n","\n","for dict in get_combined_clear_train_dicts()[:4]:\n","    print(dict)\n","for dict in get_combined_clear_train_dicts()[-4:]:\n","    print(dict)\n","    \n","print(len(get_combined_clear_train_dicts())) # 2990 + 2276 = 5266\n","print(len(get_combined_clear_val_dicts()))   # 1173 + 699  = 1872\n","\n","print('\\n**************************************************')\n","print('Combined rain')\n","def get_combined_rain_train_dicts():\n","    return get_carla_rain_train_dicts() + get_cityscapes_rain_train_dicts()\n","\n","def get_combined_rain_val_dicts():\n","    return get_carla_rain_val_dicts() + get_cityscapes_rain_val_dicts()\n","\n","for dict in get_combined_rain_train_dicts()[:4]:\n","    print(dict)\n","for dict in get_combined_rain_train_dicts()[-4:]:\n","    print(dict)\n","    \n","print(len(get_combined_rain_train_dicts())) # 2990 + 2276 = 5266\n","print(len(get_combined_rain_val_dicts()))   # 1173 + 699  = 1872\n","\n","print('\\n**************************************************')\n","print('Combined clear + rain')\n","def get_combined_all_train_dicts():\n","    return get_carla_all_train_dicts() + get_cityscapes_all_train_dicts()\n","\n","def get_combined_all_val_dicts():\n","    return get_carla_all_val_dicts() + get_cityscapes_all_val_dicts()\n","\n","for dict in get_combined_all_train_dicts()[:4]:\n","    print(dict)\n","for dict in get_combined_all_train_dicts()[-4:]:\n","    print(dict)\n","\n","print(len(get_combined_all_train_dicts())) # 5266*2 = 10532\n","print(len(get_combined_all_val_dicts()))   # 1872*2 = 3744"],"id":"8wECyKDe9bCj"},{"cell_type":"markdown","metadata":{"id":"KVqmnxQOn4Nm"},"source":["# Convert labels to train IDs\n","Execute only once"],"id":"KVqmnxQOn4Nm"},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1650042383583,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"d467859a-d17d-481f-aa6f-553a513bdfc7"},"outputs":[],"source":["# %cd '/home/tunx404/Miscellaneous/data/Cityscapes/mapped_labels/train/'\n","# for city_name in city_name_list:\n","#     !mkdir $city_name"],"id":"d467859a-d17d-481f-aa6f-553a513bdfc7"},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1650042383584,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"80ddac12"},"outputs":[],"source":["map_cityscapes_id_to_carla_id = { \n","     0:  0, # unlabeled           \n","     1:  0, # ego vehicle         \n","     2:  0, # rectification border\n","     3:  0, # out of roi          \n","     4: 19, # static              \n","     5: 20, # dynamic             \n","     6: 14, # ground              \n","     7:  7, # road                \n","     8:  8, # sidewalk            \n","     9:  0, # parking             \n","    10: 16, # rail track          \n","    11:  1, # building            \n","    12: 11, # wall                \n","    13:  2, # fence               \n","    14: 17, # guard rail          \n","    15: 15, # bridge              \n","    16:  0, # tunnel              \n","    17:  5, # pole                \n","    18:  0, # polegroup           \n","    19: 18, # traffic light       \n","    20: 12, # traffic sign        \n","    21:  9, # vegetation          \n","    22: 22, # terrain             \n","    23: 13, # sky                 \n","    24:  4, # person              \n","    25:  4, # rider               \n","    26: 10, # car                 \n","    27: 10, # truck               \n","    28: 10, # bus                 \n","    29:  0, # caravan             \n","    30:  0, # trailer             \n","    31: 10, # train               \n","    32: 10, # motorcycle          \n","    33: 10, # bicycle             \n","    -1:  0  # license plate       \n","}\n","\n","map_carla_id_to_train_id = {\n","    0 :  0, # Unlabeled    # Unlabeled    # (0, 0, 0)\n","    1 :  1, # Building     # Building     # (70, 70, 70)\n","    2 :  2, # Fence        # Fence        # (100, 40, 40)\n","    3 :  0, # Other        #\n","    4 :  3, # Pedestrian   # Pedestrian   # (220, 20, 60)\n","    5 :  4, # Pole         # Pole         # (153, 153, 153)\n","    6 :  5, # RoadLine     #\n","    7 :  5, # Road         # Road         # (128, 64, 128)\n","    8 :  6, # SideWalk     # SideWalk     # (244, 35, 232)\n","    9 :  7, # Vegetation   # Vegetation   # (107, 142, 35)\n","    10:  8, # Vehicles     # Vehicles     # (0, 0, 142)\n","    11:  9, # Wall         # Wall         # (102, 102, 156)\n","    12: 10, # TrafficSign  # TrafficSign  # (220, 220, 0)\n","    13: 11, # Sky          # Sky          # (70, 130, 180)\n","    14:  0, # Ground       #\n","    15:  0, # Bridge       #\n","    16:  0, # RailTrack    #\n","    17:  0, # GuardRail    #\n","    18: 12, # TrafficLight # TrafficLight # (250, 170, 30)\n","    19:  0, # Static       #\n","    20:  0, # Dynamic      #\n","    21:  0, # Water        #\n","    22: 13  # Terrain      # Terrain      # (145, 170, 100)\n","}\n","\n","def encode_labels(mask, map):\n","    label_mask = np.zeros_like(mask)\n","    for k in map:\n","        label_mask[mask == k] = map[k]\n","    return label_mask\n","\n","def convert_carla(file_list, data_dir):\n","    for i in tqdm(range(len(file_list))):\n","        file_id, level, package = file_list[i]\n","        image_semantic_path = os.path.join(data_dir, package, file_id + '_semantic.png')\n","        image_semantic = cv2.imread(image_semantic_path)[:, :, 2] # HxWxC, BGR\n","        image_semantic = encode_labels(image_semantic, map=map_carla_id_to_train_id)\n","        \n","        # print(image_semantic_path)\n","        # imshow_jupyter(image_semantic)\n","        \n","        output_file_name = os.path.join(data_dir, package, file_id + '_semantic_train.png')\n","        if not os.path.exists(output_file_name):\n","            # print(output_file_name)\n","            cv2.imwrite(output_file_name, image_semantic)\n","            \n","def convert_cityscapes(file_list, anno_dir):\n","    for i in tqdm(range(len(file_list))):\n","        file_name, city = file_list[i]\n","        file_name_split = file_name.split('_')\n","        \n","        output_dir = '/home/tunx404/Miscellaneous/data/Cityscapes/mapped_labels/train/'\n","        output_file_name = file_name_split[0] + '_' + file_name_split[1] + '_' + file_name_split[2] + '_train.png'\n","        output_file_name = os.path.join(output_dir, city, output_file_name)\n","        \n","        if not os.path.exists(output_file_name):\n","            anno_name = file_name_split[0] + '_' + file_name_split[1] + '_' + file_name_split[2] + '_gtFine_labelIds.png'\n","            image_semantic_path = os.path.join(anno_dir, city, anno_name)\n","            image_semantic = cv2.imread(image_semantic_path)[:, :, 0] # HxWxC, BGR\n","            image_semantic = encode_labels(image_semantic, map=map_cityscapes_id_to_carla_id)\n","            image_semantic = encode_labels(image_semantic, map=map_carla_id_to_train_id)\n","        \n","            # print(image_semantic)\n","            # imshow_jupyter(image_semantic)\n","            \n","            # print(output_file_name)\n","            cv2.imwrite(output_file_name, image_semantic)\n","\n","# convert_carla(data_carla_train_file_list, data_carla_dir)\n","# convert_carla(data_carla_val_file_list, data_carla_dir)\n","\n","# convert_cityscapes(data_cityscapes_train_file_list, anno_dir_cityscapes)\n","# convert_cityscapes(data_cityscapes_val_file_list, anno_dir_cityscapes)"],"id":"80ddac12"},{"cell_type":"markdown","metadata":{"id":"25818909"},"source":["# Datasets"],"id":"25818909"},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1650042383585,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"},"user_tz":240},"id":"33d9e8dd","outputId":"c99fa811-fdd7-4614-f1eb-4cda349bf80b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Combined clear dataset\n","Number of classes: 14\n","Number of train images: 5266\n","Number of val images:   1872\n","\n","Combined rain dataset\n","Number of classes: 14\n","Number of train images: 5266\n","Number of val images:   1872\n","\n","Combined clear + rain dataset\n","Number of classes: 14\n","Number of train images: 10532\n","Number of val images:   3744\n"]}],"source":["class Detectron2CustomDataset:\n","    train_classes = ['Unlabeled', 'Building', 'Fence', 'Pedestrian', 'Pole', 'Road', 'SideWalk', 'Vegetation', 'Vehicles', 'Wall', 'TrafficSign', 'Sky', 'TrafficLight', 'Terrain']\n","    train_colors = [(0, 0, 0), (70, 70, 70), (100, 40, 40), (220, 20, 60), (153, 153, 153), (128, 64, 128), (244, 35, 232), (107, 142, 35), (0, 0, 142), (102, 102, 156), (220, 220, 0), (70, 130, 180), (250, 170, 30), (145, 170, 100)]\n","    \n","    def __init__(self, train_dataset_name, val_dataset_name, get_train_dicts_fn, get_val_dicts_fn, classes=None, colors=None, ignore_label=0):\n","        \n","        if classes is None:\n","            self.classes = self.train_classes\n","        if colors is None:\n","            self.colors = self.train_colors\n","        print(f'Number of classes: {len(self.classes)}')\n","        \n","        self.train_dataset_name = train_dataset_name\n","        self.val_dataset_name = val_dataset_name\n","        \n","        self.get_train_dicts_fn = get_train_dicts_fn\n","        self.get_val_dicts_fn = get_val_dicts_fn\n","\n","        print(f'Number of train images: {len(self.get_train_dicts_fn())}')\n","        print(f'Number of val images:   {len(self.get_val_dicts_fn())}')\n","        \n","        DatasetCatalog.register(self.train_dataset_name, self.get_train_dicts_fn)\n","        DatasetCatalog.register(self.val_dataset_name, self.get_val_dicts_fn)\n","\n","        MetadataCatalog.get(self.train_dataset_name).stuff_classes = self.classes\n","        MetadataCatalog.get(self.train_dataset_name).stuff_colors = self.colors\n","        MetadataCatalog.get(self.train_dataset_name).ignore_label = 0\n","\n","        MetadataCatalog.get(self.val_dataset_name).stuff_classes = self.classes\n","        MetadataCatalog.get(self.val_dataset_name).stuff_colors = self.colors\n","        MetadataCatalog.get(self.val_dataset_name).ignore_label = ignore_label\n","        \n","    def visualize_train_dataset(self, num_samples=1, size=(12, 6)):\n","        train_metadata = MetadataCatalog.get(self.train_dataset_name)\n","        data_train_dicts = self.get_train_dicts_fn()\n","\n","        for file_dict in random.sample(data_train_dicts, num_samples):\n","            image = cv2.imread(file_dict['file_name'])\n","            visualizer = Visualizer(image[:, :, ::-1], metadata=train_metadata, scale=0.5)\n","            output = visualizer.draw_dataset_dict(file_dict)\n","            image = cv2.cvtColor(output.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n","            imshow_jupyter(image, size=size)\n","\n","    def visualize_val_dataset(self, predictor, num_samples=1, size=(12, 6)):\n","        val_metadata = MetadataCatalog.get(self.val_dataset_name)\n","        data_val_dicts = self.get_val_dicts_fn()\n","\n","        for file_dict in random.sample(data_val_dicts, num_samples):\n","            image = cv2.imread(file_dict['file_name'])\n","            visualizer = Visualizer(image[:, :, ::-1], metadata=val_metadata, scale=0.5)\n","            \n","            print('Ground truth')\n","            output = visualizer.draw_dataset_dict(file_dict)\n","            target_image = cv2.cvtColor(output.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n","            imshow_jupyter(target_image)\n","            \n","            print('Predicted')\n","            outputs = predictor(image)\n","            sem_seg = torch.argmax(outputs['sem_seg'], dim=0)\n","            output = visualizer.draw_sem_seg(sem_seg.to('cpu'))\n","            predicted_image = cv2.cvtColor(output.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB)\n","            imshow_jupyter(predicted_image)\n","            \n","            print()\n","\n","DatasetCatalog.clear()\n","\n","# print('Carla dataset')\n","# carla_all_dataset = Detectron2CustomDataset('carla_train', 'carla_val', get_carla_train_dicts, get_carla_val_dicts)\n","# # carla_all_dataset.visualize_train_dataset(num_samples=1, size=(20, 10))\n","            \n","# print('\\nCityscapes dataset')\n","# cityscapes_all_dataset = Detectron2CustomDataset('cityscapes_train', 'cityscapes_val', get_cityscapes_train_dicts, get_cityscapes_val_dicts)\n","# # cityscapes_dataset.visualize_train_dataset(num_samples=1, size=(20, 10))\n","            \n","print('\\nCombined clear dataset')\n","combined_clear_dataset = Detectron2CustomDataset('combined_clear_train', 'combined_clear_val', get_combined_clear_train_dicts, get_combined_clear_val_dicts)\n","# combined_clear_dataset.visualize_train_dataset(num_samples=4, size=(20, 10))\n","            \n","print('\\nCombined rain dataset')\n","combined_rain_dataset = Detectron2CustomDataset('combined_rain_train', 'combined_rain_val', get_combined_rain_train_dicts, get_combined_rain_val_dicts)\n","# combined_rain_dataset.visualize_train_dataset(num_samples=4, size=(20, 10))\n","            \n","print('\\nCombined clear + rain dataset')\n","combined_all_dataset = Detectron2CustomDataset('combined_all_train', 'combined_all_val', get_combined_all_train_dicts, get_combined_all_val_dicts)\n","# combined_all_dataset.visualize_train_dataset(num_samples=4, size=(20, 10))"],"id":"33d9e8dd"},{"cell_type":"markdown","source":["# Trainer class"],"metadata":{"id":"Z8B0gxxcEcdt"},"id":"Z8B0gxxcEcdt"},{"cell_type":"code","execution_count":13,"metadata":{"id":"b6b5d45e","executionInfo":{"status":"ok","timestamp":1650042383587,"user_tz":240,"elapsed":13,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"}}},"outputs":[],"source":["class MyTrainer(DefaultTrainer):\n","    @classmethod\n","    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n","        if output_folder is None:\n","            output_folder = os.path.join(cfg.OUTPUT_DIR, 'inference')\n","        return SemSegEvaluator(dataset_name, output_folder)\n","\n","class Detectron2Trainer:\n","    def __init__(self, train_dataset_name, val_dataset_name, output_folder):\n","        \n","        config_path = 'configs/Misc/semantic_R_50_FPN_1x.yaml'\n","        # model_path = 'https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MSRA/R-50.pkl'\n","\n","        # All configs: https://detectron2.readthedocs.io/en/latest/modules/config.html\n","        \n","        self.train_dataset_name = train_dataset_name\n","        self.val_dataset_name = val_dataset_name\n","\n","        self.cfg = get_cfg()\n","        self.cfg.merge_from_file(config_path)\n","\n","        # self.cfg.MODEL.WEIGHTS = model_path\n","        self.cfg.MODEL.DEVICE = 'cuda'\n","        # self.cfg.MODEL.DEVICE = 'cpu'\n","\n","        self.cfg.DATASETS.TRAIN = (self.train_dataset_name,)\n","        # self.cfg.DATASETS.TEST = (self.train_dataset_name,)\n","        self.cfg.DATASETS.TEST = (self.val_dataset_name,)\n","\n","        self.cfg.DATALOADER.NUM_WORKERS = 2\n","\n","        self.cfg.INPUT.MIN_SIZE_TRAIN = 720\n","        self.cfg.INPUT.MAX_SIZE_TRAIN = 2048\n","        self.cfg.INPUT.MIN_SIZE_TEST = 720\n","        self.cfg.INPUT.MAX_SIZE_TEST = 2048\n","\n","        # Number of images per batch across all machines. This is also the number\n","        # of training images per step (i.e. per iteration).\n","        self.cfg.SOLVER.IMS_PER_BATCH = 8\n","        self.cfg.SOLVER.BASE_LR = 0.01\n","        self.cfg.SOLVER.MAX_ITER = 20000\n","        self.cfg.SOLVER.GAMMA = 0.1\n","        # The iteration number to decrease learning rate by GAMMA.\n","        self.cfg.SOLVER.STEPS = (10000, 15000, 18000, 19000)\n","        # Save a checkpoint after every this number of iterations\n","        self.cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n","\n","        self.cfg.TEST.EVAL_PERIOD = 1000\n","\n","        self.cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE = 0\n","        classes = MetadataCatalog.get(self.train_dataset_name).stuff_classes\n","        self.cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = len(classes)\n","        \n","        # Directory where output files are written\n","        self.cfg.OUTPUT_DIR = output_folder\n","        os.makedirs(self.cfg.OUTPUT_DIR, exist_ok=True)\n","\n","        # self.trainer = DefaultTrainer(self.cfg)\n","        self.trainer = MyTrainer(self.cfg)\n","        \n","        self.evaluator = None\n","        self.predictor = None\n","        \n","    def load(self):\n","        self.trainer.resume_or_load(resume=True)\n","        \n","    def train(self):\n","        self.trainer.train()\n","        \n","    def get_predictor(self, output_folder=None):\n","        if output_folder is None:\n","            output_folder = self.cfg.OUTPUT_DIR\n","        last_checkpoint = 'model_final.pth'\n","        with open(os.path.join(output_folder, 'last_checkpoint')) as file:\n","            last_checkpoint = file.read()\n","        print('Last checkpoint: ' + last_checkpoint)\n","            \n","        self.cfg.MODEL.WEIGHTS = os.path.join(self.cfg.OUTPUT_DIR, last_checkpoint)\n","        self.predictor = DefaultPredictor(self.cfg)\n","        \n","    def test(self, output_folder=None):\n","        if output_folder is None:\n","            output_folder = self.cfg.OUTPUT_DIR\n","        self.get_predictor(output_folder)\n","        self.evaluator = SemSegEvaluator(self.val_dataset_name, output_dir=os.path.join(output_folder, 'inference'))\n","        self.trainer.test(self.cfg, self.predictor.model, self.evaluator)"],"id":"b6b5d45e"},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"l0TtWq0-DxjT"},"id":"l0TtWq0-DxjT"},{"cell_type":"code","execution_count":14,"metadata":{"id":"6bd2452e","executionInfo":{"status":"ok","timestamp":1650042383588,"user_tz":240,"elapsed":13,"user":{"displayName":"Tũn Tũn","userId":"10147808466179585969"}}},"outputs":[],"source":["# trainer_all = Detectron2Trainer('combined_all_train', 'combined_all_val', output_folder='./output_all')\n","# trainer_all.load()\n","# trainer_all.train()\n","# trainer_all.test()"],"id":"6bd2452e"},{"cell_type":"code","source":["trainer_clear = Detectron2Trainer('combined_clear_train', 'combined_clear_val', output_folder='./output_clear')\n","trainer_clear.load()\n","trainer_clear.train()\n","trainer_clear.test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPFpcmeBIxQQ","outputId":"20e4d59b-6ed2-429c-d005-b9a344661890"},"id":"mPFpcmeBIxQQ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m[04/15 17:07:03 d2.engine.defaults]: \u001b[0mModel:\n","SemanticSegmentor(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (sem_seg_head): SemSegFPNHead(\n","    (p2): Sequential(\n","      (0): Conv2d(\n","        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","    )\n","    (p3): Sequential(\n","      (0): Conv2d(\n","        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (1): Upsample(scale_factor=2.0, mode=bilinear)\n","    )\n","    (p4): Sequential(\n","      (0): Conv2d(\n","        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (1): Upsample(scale_factor=2.0, mode=bilinear)\n","      (2): Conv2d(\n","        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (3): Upsample(scale_factor=2.0, mode=bilinear)\n","    )\n","    (p5): Sequential(\n","      (0): Conv2d(\n","        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (1): Upsample(scale_factor=2.0, mode=bilinear)\n","      (2): Conv2d(\n","        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (3): Upsample(scale_factor=2.0, mode=bilinear)\n","      (4): Conv2d(\n","        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (5): Upsample(scale_factor=2.0, mode=bilinear)\n","    )\n","    (predictor): Conv2d(128, 14, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")\n","\u001b[32m[04/15 17:07:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(720, 720), max_size=2048, sample_style='choice'), RandomFlip()]\n","\u001b[32m[04/15 17:07:04 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[04/15 17:07:04 d2.data.common]: \u001b[0mSerializing 5266 elements to byte tensors and concatenating them all ...\n","\u001b[32m[04/15 17:07:04 d2.data.common]: \u001b[0mSerialized dataset takes 1.37 MiB\n","\u001b[32m[04/15 17:07:10 d2.checkpoint.c2_model_loading]: \u001b[0mRenaming Caffe2 weights ......\n","\u001b[32m[04/15 17:07:10 d2.checkpoint.c2_model_loading]: \u001b[0mFollowing weights matched with submodule backbone.bottom_up:\n","| Names in Model    | Names in Checkpoint      | Shapes                                          |\n","|:------------------|:-------------------------|:------------------------------------------------|\n","| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,1,1)             |\n","| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n","| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n","| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n","| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n","| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n","| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n","| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w} | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n","| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w} | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n","| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w} | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n","| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,256,1,1)       |\n","| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n","| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n","| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,256,1,1)       |\n","| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n","| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n","| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n","| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n","| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n","| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n","| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w} | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n","| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w} | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n","| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w} | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n","| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,512,1,1)       |\n","| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n","| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n","| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |\n","| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n","| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n","| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n","| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n","| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n","| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n","| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n","| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n","| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n","| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n","| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n","| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n","| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n","| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n","| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n","| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,1024,1,1)      |\n","| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n","| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n","| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |\n","| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n","| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n","| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n","| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w} | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n","| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w} | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n","| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w} | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n","| stem.conv1.norm.* | res_conv1_bn_*           | (64,) (64,) (64,) (64,)                         |\n","| stem.conv1.weight | conv1_w                  | (64, 3, 7, 7)                                   |\n"]},{"output_type":"stream","name":"stderr","text":["Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mbackbone.fpn_lateral2.{bias, weight}\u001b[0m\n","\u001b[34mbackbone.fpn_lateral3.{bias, weight}\u001b[0m\n","\u001b[34mbackbone.fpn_lateral4.{bias, weight}\u001b[0m\n","\u001b[34mbackbone.fpn_lateral5.{bias, weight}\u001b[0m\n","\u001b[34mbackbone.fpn_output2.{bias, weight}\u001b[0m\n","\u001b[34mbackbone.fpn_output3.{bias, weight}\u001b[0m\n","\u001b[34mbackbone.fpn_output4.{bias, weight}\u001b[0m\n","\u001b[34mbackbone.fpn_output5.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p2.0.norm.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p2.0.weight\u001b[0m\n","\u001b[34msem_seg_head.p3.0.norm.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p3.0.weight\u001b[0m\n","\u001b[34msem_seg_head.p4.0.norm.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p4.0.weight\u001b[0m\n","\u001b[34msem_seg_head.p4.2.norm.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p4.2.weight\u001b[0m\n","\u001b[34msem_seg_head.p5.0.norm.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p5.0.weight\u001b[0m\n","\u001b[34msem_seg_head.p5.2.norm.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p5.2.weight\u001b[0m\n","\u001b[34msem_seg_head.p5.4.norm.{bias, weight}\u001b[0m\n","\u001b[34msem_seg_head.p5.4.weight\u001b[0m\n","\u001b[34msem_seg_head.predictor.{bias, weight}\u001b[0m\n","The checkpoint state_dict contains keys that are not used by the model:\n","  \u001b[35mfc1000.{bias, weight}\u001b[0m\n","  \u001b[35mstem.conv1.bias\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m[04/15 17:07:11 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  max_size = (max_size + (stride - 1)) // stride * stride\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m[04/15 17:07:42 d2.utils.events]: \u001b[0m eta: 8:11:57  iter: 19  total_loss: 5.526  time: 1.4971  data_time: 0.4945  lr: 0.00019981  max_mem: 8441M\n","\u001b[32m[04/15 17:08:12 d2.utils.events]: \u001b[0m eta: 8:16:11  iter: 39  total_loss: 1.128  time: 1.5174  data_time: 0.4929  lr: 0.00039961  max_mem: 8441M\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39c385b8"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir output_all"],"id":"39c385b8"},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir output_clear"],"metadata":{"id":"aVfRixZjJS_a"},"id":"aVfRixZjJS_a","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluate"],"metadata":{"id":"t5iO65WNDzRa"},"id":"t5iO65WNDzRa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"07d49a7f"},"outputs":[],"source":["# trainer_all = Detectron2Trainer('combined_all_train', 'combined_all_val', output_folder='./output_all')\n","# trainer_all.test()"],"id":"07d49a7f"},{"cell_type":"code","source":["# trainer_rain = Detectron2Trainer('combined_rain_train', 'combined_rain_val', output_folder='./output_all')\n","# trainer_rain.test()"],"metadata":{"id":"qyXzC8KvEBHx"},"id":"qyXzC8KvEBHx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trainer_clear = Detectron2Trainer('combined_clear_train', 'combined_clear_val', output_folder='./output_all')\n","# trainer_clear.test()"],"metadata":{"id":"PlZ3ag3oEJoE"},"id":"PlZ3ag3oEJoE","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9032c440"},"outputs":[],"source":["# trainer_rain.get_predictor()\n","# combined_rain_dataset.visualize_val_dataset(trainer_rain.predictor, num_samples=5)"],"id":"9032c440"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["KVqmnxQOn4Nm"],"name":"detectron2_sem_seg_trainer.ipynb","provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":5}