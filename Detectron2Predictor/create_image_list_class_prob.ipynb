{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOI__nJ2oiYZ"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "566a15a0"
   },
   "outputs": [],
   "source": [
    "main_dir = './'\n",
    "data_dir = '/home/gregory/Documents/RainPerception/' # Local Jupyter\n",
    "\n",
    "from utilities import create_file_list, imshow_jupyter\n",
    "# import carla_converter\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJEp7LRapA9e"
   },
   "source": [
    "## Carla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1650052929021,
     "user": {
      "displayName": "Tũn Tũn",
      "userId": "10147808466179585969"
     },
     "user_tz": 240
    },
    "id": "16e51555",
    "outputId": "1ebb63e8-2a4c-4044-de06-030918e8b7fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4163it [00:00, 160993.64it/s]\n"
     ]
    }
   ],
   "source": [
    "data_carla_dir = data_dir + 'Carla/packaging/'\n",
    "\n",
    "# Dir structure:\n",
    "# <data_carla_dir>\n",
    "#     packages2\n",
    "#     packages3\n",
    "#     ...\n",
    "\n",
    "def get_carla_file_list(data_dir, packages=[], levels=[]):\n",
    "    file_list = []\n",
    "    \n",
    "    for package in packages:\n",
    "        temp_file_name_list, temp_file_path_list = create_file_list(data_dir, package)\n",
    "        \n",
    "        for i in range(len(temp_file_name_list)):\n",
    "            file_name_split = temp_file_name_list[i].split('_')\n",
    "            if len(file_name_split) == 3: # [id, type, level.png]\n",
    "                file_id = file_name_split[0]\n",
    "                level = file_name_split[2].replace('.png', '')\n",
    "                if level in levels:\n",
    "                    file_list.append((file_id, level, package))\n",
    "                    \n",
    "    print(f'Number of images: {len(file_list)}')\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "# data_carla_train_file_list = get_carla_file_list(data_carla_dir, packages=['package2', 'package3', 'package4', 'package5', 'package6', 'package7', 'package9'], levels=['H', 'M', 'S'])\n",
    "# data_carla_val_file_list = get_carla_file_list(data_carla_dir, packages=['package8'], levels=['H', 'M', 'S'])\n",
    "\n",
    "data_carla_train_file_list = get_carla_file_list(data_carla_dir, packages=['package2', 'package3', 'package4', 'package5', 'package6', 'package7', 'package8', 'package9'], levels=['H', 'M', 'S'])\n",
    "\n",
    "# # Toy example\n",
    "# data_carla_train_file_list = data_carla_train_file_list[:10]\n",
    "# data_carla_val_file_list = data_carla_val_file_list[:10]\n",
    "\n",
    "def get_carla_list_files(file_list, dataset_type='train'):\n",
    "    image_list_file = open('image_list/' + dataset_type + '.txt', mode='w')\n",
    "    image_csv_writer = csv.writer(image_list_file, delimiter=',')\n",
    "    \n",
    "    label_list_file = open('label_list/' + dataset_type + '.txt', mode='w')\n",
    "    label_csv_writer = csv.writer(label_list_file, delimiter=',')\n",
    "    \n",
    "    for index, file in tqdm(enumerate(file_list)):\n",
    "        file_id, level, package = file\n",
    "        \n",
    "        image_path = os.path.join('packaging/', package, file_id + '_clear.png')\n",
    "        image_semantic_path = os.path.join('packaging/', package, file_id + '_semantic_train.png')\n",
    "        \n",
    "        image_csv_writer.writerow([image_path])\n",
    "        label_csv_writer.writerow([image_semantic_path])\n",
    "\n",
    "get_carla_list_files(data_carla_train_file_list, dataset_type='train')\n",
    "# get_carla_list_files(data_carla_val_file_list, dataset_type='val')\n",
    "\n",
    "import pickle\n",
    "def get_carla_class_pixel_count(file_list, data_dir):\n",
    "    train_classes = ['Unlabeled', 'Building', 'Fence', 'Pedestrian', 'Pole', 'Road', 'SideWalk', 'Vegetation', 'Vehicles', 'Wall', 'TrafficSign', 'Sky', 'TrafficLight', 'Terrain']\n",
    "    num_classes = len(train_classes)\n",
    "    print(f'Number of images: {len(file_list)}')\n",
    "    print(f'Number of classes: {num_classes}')\n",
    "    \n",
    "    class_pixel_count_dict = {}\n",
    "    for i in range(num_classes):\n",
    "        class_pixel_count_dict[train_classes[i]] = 0\n",
    "        \n",
    "    class_pixel_count_per_sample_list = []\n",
    "    \n",
    "    for index, file in tqdm(enumerate(file_list)):\n",
    "        file_id, level, package = file\n",
    "        \n",
    "        image_path = os.path.join('packaging/', package, file_id + '_clear.png')\n",
    "        image_semantic_path = os.path.join(data_dir, package, file_id + '_semantic_train.png')\n",
    "        image_semantic = cv2.imread(image_semantic_path)[:, :, 2] # HxWxC, BGR\n",
    "        \n",
    "        # print(image_semantic)\n",
    "        # imshow_jupyter(image_semantic)\n",
    "        \n",
    "        class_pixel_count_dict_temp = {}\n",
    "        \n",
    "        for i in range(num_classes):\n",
    "            class_pixel_count_dict[train_classes[i]] += np.sum(image_semantic == i)\n",
    "            class_pixel_count_dict_temp[train_classes[i]] = np.sum(image_semantic == i)\n",
    "            \n",
    "        class_pixel_count_per_sample_list.append(class_pixel_count_dict_temp)\n",
    "    \n",
    "    with open('class_pixel_count_dict.pickle', 'wb') as file:\n",
    "        pickle.dump(class_pixel_count_dict, file)\n",
    "    \n",
    "    with open('class_pixel_count_per_sample_list.pickle', 'wb') as file:\n",
    "        pickle.dump(class_pixel_count_per_sample_list, file)\n",
    "        \n",
    "    return class_pixel_count_dict, class_pixel_count_per_sample_list\n",
    "\n",
    "# train_classes = ['Unlabeled', 'Building', 'Fence', 'Pedestrian', 'Pole', 'Road', 'SideWalk', 'Vegetation', 'Vehicles', 'Wall', 'TrafficSign', 'Sky', 'TrafficLight', 'Terrain']\n",
    "# num_classes = len(train_classes)\n",
    "\n",
    "# class_pixel_count_carla_train_dict, class_pixel_count_per_sample_carla_train_list = get_carla_class_pixel_count(data_carla_train_file_list, data_carla_dir)\n",
    "# print(class_pixel_count_carla_train_dict)\n",
    "\n",
    "# temp_dict = {}\n",
    "# for i in range(num_classes):\n",
    "#     temp_dict[train_classes[i]] = 0\n",
    "# for index, item in tqdm(enumerate(class_pixel_count_per_sample_carla_train_list)):\n",
    "#     for i in range(num_classes):\n",
    "#         # print(temp_dict[train_classes[i]])\n",
    "#         temp_dict[train_classes[i]] += item[train_classes[i]]\n",
    "# print(temp_dict)\n",
    "# pixel_sum = sum(temp_dict.values())\n",
    "# print(pixel_sum) # 2755584000 # 3836620800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'class_pixel_count_dict.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22570/2259551406.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class_pixel_count_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mclass_pixel_count_carla_train_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_pixel_count_carla_train_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpixel_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_pixel_count_carla_train_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'class_pixel_count_dict.pickle'"
     ]
    }
   ],
   "source": [
    "with open('class_pixel_count_dict.pickle', 'rb') as file:\n",
    "    class_pixel_count_carla_train_dict = pickle.load(file)\n",
    "print(class_pixel_count_carla_train_dict)\n",
    "pixel_sum = sum(class_pixel_count_carla_train_dict.values())\n",
    "\n",
    "print(pixel_sum) # 2755584000 # 3836620800\n",
    "assert pixel_sum == len(data_carla_train_file_list)*1280*720\n",
    "\n",
    "# non_ignored_sum = sum(class_pixel_count_carla_train_dict.values()) - class_pixel_count_carla_train_dict[train_classes[0]]\n",
    "# print(non_ignored_sum) # 2642685163\n",
    "\n",
    "class_freq_carla_train_dict = {}\n",
    "\n",
    "# # Ignore class 0\n",
    "# class_freq_carla_train_dict[0] = 0.0\n",
    "# for i in range(1, num_classes):\n",
    "#     class_freq_carla_train_dict[i] = class_pixel_count_carla_train_dict[i]/non_ignored_sum\n",
    "    \n",
    "# Include all\n",
    "for i in range(num_classes):\n",
    "    class_freq_carla_train_dict[train_classes[i]] = class_pixel_count_carla_train_dict[train_classes[i]]/pixel_sum\n",
    "    \n",
    "print(class_freq_carla_train_dict)\n",
    "print(sum(class_freq_carla_train_dict.values())) # 1.0\n",
    "with open('class_freq_dict.pickle', 'wb') as file:\n",
    "    pickle.dump(class_freq_carla_train_dict, file)\n",
    "    \n",
    "with open('class_freq_dict.pickle', 'rb') as file:\n",
    "    x = pickle.load(file)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEjETjkApDCI"
   },
   "source": [
    "## Cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1650052929299,
     "user": {
      "displayName": "Tũn Tũn",
      "userId": "10147808466179585969"
     },
     "user_tz": 240
    },
    "id": "0c8d50c8-2f3a-4e2e-9d35-1a8e7d47d7b8",
    "outputId": "7ca806c2-dee9-4181-fa80-12fc9562bbc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cities: 18\n",
      "Number of images: 2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2975it [00:00, 159708.87it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir_cityscapes = data_dir + 'Cityscapes/leftImg8bit/train/'\n",
    "data_dir_rain_addition = data_dir + 'RainAddition/train/'\n",
    "# anno_dir_cityscapes = data_dir + 'Cityscapes/gtFine/train/'\n",
    "anno_dir_cityscapes = data_dir + 'Cityscapes/mapped_labels/train/'\n",
    "\n",
    "city_name_list, _ = create_file_list(data_dir_cityscapes)\n",
    "print(f'Number of cities: {len(city_name_list)}')\n",
    "\n",
    "def get_cityscapes_file_list(data_dir, cities=[], levels=[]):\n",
    "    file_list = []\n",
    "    \n",
    "    for city in cities:\n",
    "        temp_file_name_list, _ = create_file_list(data_dir, city)\n",
    "        \n",
    "        for file_name in temp_file_name_list:\n",
    "            file_list.append((file_name, city))\n",
    "                    \n",
    "    print(f'Number of images: {len(file_list)}')\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "# data_cityscapes_train_file_list = get_cityscapes_file_list(data_dir_cityscapes, cities=city_name_list[:13], levels=['H', 'M', 'S'])\n",
    "# data_cityscapes_val_file_list   = get_cityscapes_file_list(data_dir_cityscapes, cities=city_name_list[13:], levels=['H', 'M', 'S'])\n",
    "\n",
    "\n",
    "data_cityscapes_train_file_list = get_cityscapes_file_list(data_dir_cityscapes, cities=city_name_list, levels=['H', 'M', 'S'])\n",
    "\n",
    "# # Toy example\n",
    "# data_cityscapes_train_file_list = data_cityscapes_train_file_list[:10]\n",
    "# data_cityscapes_val_file_list = data_cityscapes_val_file_list[:10]\n",
    "\n",
    "def get_cityscapes_list_files(file_list, dataset_type='train'):\n",
    "\n",
    "    image_list_file = open('image_list/' + dataset_type + '.txt', mode='w')\n",
    "    image_csv_writer = csv.writer(image_list_file, delimiter=',')\n",
    "    \n",
    "    label_list_file = open('label_list/' + dataset_type + '.txt', mode='w')\n",
    "    label_csv_writer = csv.writer(label_list_file, delimiter=',')\n",
    "    \n",
    "    for index, file in tqdm(enumerate(file_list)):\n",
    "        \n",
    "        file_name, city = file\n",
    "\n",
    "        image_path = os.path.join(city, file_name)\n",
    "        \n",
    "        file_name_split = file_name.split('_')\n",
    "        image_id = file_name_split[0] + '_' + file_name_split[1] + '_' + file_name_split[2]\n",
    "        anno_name = image_id + '_train.png' # Mapped\n",
    "        image_semantic_path = os.path.join(city, anno_name)\n",
    "        \n",
    "        image_csv_writer.writerow([image_path])\n",
    "        label_csv_writer.writerow([image_semantic_path])\n",
    "\n",
    "get_cityscapes_list_files(data_cityscapes_train_file_list, dataset_type='train')\n",
    "# get_cityscapes_list_files(data_cityscapes_val_file_list, dataset_type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KVqmnxQOn4Nm"
   ],
   "name": "detectron2_sem_seg_trainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
